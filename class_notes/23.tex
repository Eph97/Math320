\section{2022-12-07}

\subsection{Applications in Probability}

\begin{definition}[Finite Probability]
	\begin{align*}
		&\Omega - \{ \omega_1, \ldots , \omega_n \} \quad \text{possible outcomes} \\
		&P : \Omega \to [0,\infty) \quad \text{outcome probabilities} \\
		&\sum_{\omega \in \Omega} p(\omega) = 1
	\end{align*} 
\end{definition}

\begin{example}
	flip 3 fair coins
\end{example}

an event $E$ is a subset $E \subseteq \Omega$

\begin{example}
	\begin{align*}
		E &= \{ \text{there are two heads} \} \\
		  &= \{HHT, HTH, THH\}
	\end{align*} 
	probability of event $E \subseteq \Omega$ is $\mathbb{P}(E) = \sum_{\omega \in E} p(\omega)$
\end{example}

\begin{example}
	P(\{ there are two heads \}) = $\frac{3}{8}$
\end{example}

\begin{definition}
	A random variable is a function $V : \Omega \to \R$

	Random variables  $V$ have expectation 
	 \begin{align*}
		\E V = \sum_{\omega \in \Omega} V(\omega) p(\omega)
	\end{align*} 
\end{definition}

\begin{example}
	$V =$ number of heads, $\E V = \frac{3}{2}$
\end{example}

Expectation is linear:
\begin{align*}
	\E (V_1 + V_2) = \E V_1 + \E V_2
\end{align*} 

Two events $E_1, E_2 \subseteq \Omega$ are independent if 
\begin{align*}
	\mathbb{P} (E_1 \cap E_2 ) = \mathbb{P} (E_1) \mathbb{P} (E_2)
\end{align*} 

\begin{example}
	$E_k =$ kth flip is heads, $E_1, E_2$ are independent.
	\begin{align*}
		E_1 \cap E_2 &= \{ HHT, HHH\}, \mathbb{P}(E_1 \cap E_2 ) = \frac{1}{4}, \mathbb{P}(E_1) = \mathbb{P}(E_2) = \frac{1}{2}
	\end{align*} 
\end{example}

\begin{example}
	$V_k = \1_{E_k}$, $V_1$ and $V_2$ are independent.
	If $V_1$ and $V_2$ are independent then 
	\[
		\E(V_1 V_2) = \E(V_1) \E(V_2)
	\] 
\end{example}


Two random variables $V_1, V_2$ are independent if events $\{V_1 = K_1 \}$ and $\{V_2 = K_2 \}$ 
are independent for all $k_1 , k_2 \in \R$.

\begin{definition}[Countably Infinite Probability]
	\begin{align*}
		\Omega &= \{ \omega_k : k \in \N \} \\
		\mathbb{P} &: \Omega \to [0,\infty) \\
		\sum_{\omega \in \Omega} p(\omega) &= 1
	\end{align*}
\end{definition}


\subsection{Uncountably infinite probabilities}

How to choose random number uniformly from $[0,1]?$

\begin{remark}
	Motivating example: If $p : [0,1] \to [0,\infty)$ and $p(x) = p(y)$ for $x,y \in [0,1]$, then
	\[
		\sum_{x \in [0,1]} p(x) = 0,\text{or }\infty
	\] 
\end{remark}


\begin{definition}[Kolmogorov]
	Forget specific outcomes and deal only with "measurable events."
	
\end{definition}

\begin{definition}[Probability space]
	A Probability space is a measure space $(X,S, m)$ with $m(X)=1$
	Convenction:
	\begin{align*}
		(\Omega, \mathcal{F}, \mathbb{B}( = (X,S, m)
	\end{align*} 
\end{definition}

\begin{example}
	Choosing uniform random element of $[0,1]$:
	 \[
		 ([0,1], \B([0,1]), L)
	\] 
\end{example}

We're then allowed to as Borel questions about the outcome:
let
$X : [0,1] \to [0,1]$ be identity $X(x) = x$.

$X$ is a borel measure, so it is a random variable.

 \[
	\mathbb{P}(X = x_0) =0, \; \mathbb{P}(a \leq X \leq b) = b-a, \; 0 \leq a \leq b \leq 1
\]

\[
	\E X = \frac{1}{2}, \; \E X^2 = 1/3
\] 

The elements of $\mathcal{F}$ are measurable events. The $\mathcal{F}$-measurable functions are random variables.

Probability of $E \in \mathcal{F}$ is $\mathbb{P}(E)$. Expectation of $\mathcal{F}$-measurable $V : \Omega \to \R$ is $\E V = \int V d \mathcal{P}$, provided
\[
	\int |V| d \mathbb{P} < \infty
\] 

Independence of events is unchanged. Call $V_1, V_2$ independent if $\{ a_1 \leq V_1 \leq b_1 \}$,
$\{ a_2 \leq V_2 \leq b_2 \}$ and independence for all  $a_1 \leq b_1$ and $a_2 \leq b_2$


\begin{align*}
	\E ( V_1 + V_2 ) &= \E V_1 + \E V_2 \\
	\E (V_1 V_2 ) &= ( \E V_1 ) (\E V_2)
\end{align*} when $V_1$ and $V_2$ are independent and integrable.

\begin{definition}[Random Sequences]
	For sets $A$ and $B$ write $A^B$ for set of functions $s : B \to A$.
\end{definition}

given finite sets $A$, want to choose uniform random element of $A^{\N}$
 

To define $\sigma$-algebra, choose notion of "Basic Measurement."

For any $K \in \N$, look at value at $K.$


For each  $k \in \N$ and $a \in A$, define
\[
	E_{k,a} = \{ s \in A^{\N} : s(k) = a \},
\] 
let $\mathcal{F}$ be $\sigma$-algebra generated by $\E_{k,a}$ 
\[
	\mathcal{P}(E_{k,a}) = \frac{1}{\#A}
\] 

The $E_{k,a}$ generate the cylindrical algebra.


given finite $x \subseteq \N$ and $S \subseteq A^X$ define the cylinder set

\[
	C_{x,s} = \{ s \in A^{\N} : s |_{x} \in S
\] 
[insert image]

The cylinder sets $\{ C_{s,x} \}$ are algebra generated by $E_{k,a}$.

We know 
\[
	\mathbb{P}(C_{s,x} ) = \frac{\#S}{\# A^x}
\] 
For $E \subseteq A^{\N}$ aribitrary, let
 \[
	 \mathbb{P}(E) = \inf \{\mathbb{P}(X_{s,x}) : E \subseteq C_{s,x} \}
\] 
This is an outer measure and all cylinder sets are measurable.

Let $\mathcal{F}$ be $\sigma$-algebra generated by $E_{k,a}$
 
We then have a model for uniform random element of $A^{\N}$:
\[
	(A^{\N}, \mathcal{F}, \mathbb{P})
\] 

\begin{example}[Infinite Monkey Theorem]

	$A = $\{ printable ascii characters \}, then
	\begin{align*}
		\mathbb{P}(all of usenet appears ) = 1
	\end{align*} 
\end{example}

\begin{proof}
	Choose $n \geq 1$ large so all of usenet can appear in $A ^{\{0,\ldots,n-1\}}$,

	Choose  $r \in A^{\{0,\ldots,n-1\}} $ on which a copy of usenet appears.
	\begin{align*}
		\mathbb{P}(C_{\{0,\ldots, n-1\}, \{r\}} &> 0 \\
												&\approx 256^{-10^{12}}
	\end{align*}
	Let
	\begin{align*}
		&X_k = \{ K_n, \ldots, K_n + n-1 \} \\
		&r_k \in A^{X_k}, \\
		&r_k(K_n + j) = r(j) \\
		&C_k = C_{x_k, \{r_k\}}
	\end{align*}
	
	The $C_k$ are $\mathbb{P}$-independent.

	Compute event 
	\[
		\{\text{usenet appears \}} \contains \bigcup_{k \in \N} C_k
	\] 
	 
	compute
	\begin{align*}
		\mathbb{P}( \bigcup_{k=1}^{m} C_k ) &= \mathbb{P}((\bigcap_{k=1}^{m} C_{k}^c )^c) \\
											&= 1 - \mathbb{P} ( \bigcap_{k=1}^{m} C_{k}^c ) \\
											&= 1 - \Pi_{k=1}^{m} \mathbb{P}(C_k^c) \\
											&= 1 - \left(\mathbb{P}(C^c_{\{0, \ldots, n-1\}, \{r\}})\right)^m \\
											&= 1 - \left(1 - (\mathbb{P}(C_{\{0, \ldots, n-1\}, \{r\}})\right)^m \to 0 \quad \text{as } m\to \infty \\
											&\geq 1 - (1 - 256^{-10^12})m
	\end{align*} 

	Need $m \approx 256^{10^{12}}$ to get $\mathbb{P} = 1/2$, by limit theorems
	\[
		\mathbb{P}(\{\text{usenet appears \})} = 1
	\] 
\end{proof}

